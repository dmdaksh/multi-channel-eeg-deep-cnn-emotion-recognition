{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er2DV8rWGT45"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, AvgPool1D, Dense, Conv1D, Flatten, Dropout, Input, BatchNormalization, GlobalMaxPool1D, MaxPool1D, SpatialDropout1D, GlobalAvgPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import StandardScaler                                                      \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy import signal\n",
    "import pickle as pkl\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkyW9Mo3HWDB"
   },
   "outputs": [],
   "source": [
    "sampling_rate = 128\n",
    "window_size = 1280\n",
    "overlap = 256\n",
    "channel_len = 32\n",
    "classes=2\n",
    "\n",
    "bands = {'delta': [0.5/(sampling_rate/2), 4/(sampling_rate/2)], 'theta': [4/(sampling_rate/2), 8/(sampling_rate/2)], \\\n",
    "         'alpha': [8/(sampling_rate/2), 14/(sampling_rate/2)], 'beta': [14/(sampling_rate/2), 30/(sampling_rate/2)], \\\n",
    "         'gamma': [30/(sampling_rate/2), 75/(sampling_rate/2)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUyCKSoo5Cio"
   },
   "outputs": [],
   "source": [
    "def load_data(eeg_band = None):\n",
    "\n",
    "  eeg_signal = []\n",
    "  valence = []\n",
    "  arousal = []\n",
    "  dominance = []\n",
    "\n",
    "  for person in range(32):\n",
    "    print('Person No.' + str(person))\n",
    "    \n",
    "    # EEG files address\n",
    "    address = './eeg_data/DEAP/s'+subject[person]+'.dat'\n",
    "\n",
    "    with open(address, 'rb') as file:\n",
    "      data = pkl.load(file, encoding = 'latin1')\n",
    "\n",
    "    eeg = data['data']\n",
    "    label = data['labels']\n",
    "    \n",
    "    # Assigning classes\n",
    "    label[label<5.5] = 0\n",
    "    label[label>=5.5] = 1     \n",
    "\n",
    "    val = label.T[0] # Valence label\n",
    "    aro = label.T[1] # Arousal label\n",
    "    dom = label.T[2] # Dominance label\n",
    "\n",
    "    del data, label\n",
    "    \n",
    "\n",
    "    for i in range(40): # Iterating through 40 vidoes/trials\n",
    "\n",
    "      sig = eeg[i].T\n",
    "      v = val[i]\n",
    "      a = aro[i]\n",
    "      d = dom[i]\n",
    "\n",
    "      sig = sig[:, :32] # Considering all 32 EEG channels\n",
    "      \n",
    "      num, den = signal.butter(4, bands[eeg_band], 'band') # Butterworth filter of order N = 4\n",
    "      band_signal = signal.filtfilt(num, den, sig, axis=0)\n",
    "      sig = band_signal\n",
    "      del band_signal, num, den\n",
    "      \n",
    "      scaler = StandardScaler().fit(sig)\n",
    "      scaled_sig = scaler.transform(sig)\n",
    "\n",
    "      del sig\n",
    "        \n",
    "      # Segmenting into 10 seconds (1280 timesteps) windows with 2 seconds (256 timesteps) overlap\n",
    "      start = 0\n",
    "      while start + window_size < scaled_sig.shape[0]:\n",
    "        eeg_signal.append(scaled_sig[start:start+window_size, :])\n",
    "\n",
    "        valence.append(v)\n",
    "        arousal.append(a)\n",
    "        dominance.append(d)\n",
    "        start += overlap\n",
    "\n",
    "      del scaled_sig\n",
    "\n",
    "\n",
    "  eeg_signal = np.asarray(eeg_signal, dtype = np.float32) # Using 32 bit floating point value to save memory\n",
    "  valence = np.asarray(valence, dtype = np.int8)\n",
    "  arousal = np.asarray(arousal, dtype = np.int8)\n",
    "  dominance = np.asarray(dominance, dtype = np.int8)\n",
    "\n",
    "  print(valence.shape, valence[valence == 0].shape, valence[valence == 1].shape)\n",
    "  print(arousal.shape, arousal[arousal == 0].shape, arousal[arousal == 1].shape)\n",
    "  print(dominance.shape, dominance[dominance == 0].shape, dominance[dominance == 1].shape)\n",
    "\n",
    "  valence = np_utils.to_categorical(valence)\n",
    "  arousal = np_utils.to_categorical(arousal)\n",
    "  dominance = np_utils.to_categorical(dominance)\n",
    "\n",
    "    \n",
    "  return (eeg_signal, valence, arousal, dominance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9djyoTzAX65O",
    "outputId": "e332793b-0474-4647-9554-d67f33e3cf86"
   },
   "outputs": [],
   "source": [
    "FOLD = 10\n",
    "eeg_band = 'alpha' # EEG band name\n",
    "\n",
    "data, valence, arousal, dominance = load_data(eeg_band) # Loading processed data\n",
    "\n",
    "nb_samples = data.shape[0] # Number of samples\n",
    "factor = nb_samples//FOLD # Kth fold by this factor\n",
    " \n",
    "shuffler = np.random.permutation(nb_samples) # Shuffling data\n",
    "\n",
    "data = data[shuffler]\n",
    "valence = valence[shuffler]\n",
    "arousal = arousal[shuffler]\n",
    "dominance = dominance[shuffler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfkgqY2uOB9M"
   },
   "outputs": [],
   "source": [
    "eeg_input = Input(shape = (window_size, channel_len), name='eeg_input') # Input layer\n",
    "\n",
    "# CNN model\n",
    "def get_CNN_design():\n",
    "  x = Conv1D(filters = 32, kernel_size = 1, strides = 2, padding = 'valid', activation='relu', name='conv1')(eeg_input)\n",
    "  x = Conv1D(filters = 32, kernel_size = 5, strides = 2, padding = 'valid', activation='relu', name='conv2')(x)\n",
    "  x = AvgPool1D(pool_size=2, name='avg_pool1')(x)\n",
    "  x = BatchNormalization(name='batch_norm1')(x)\n",
    "  x = SpatialDropout1D(rate=0.0625, name = 'spatial_dropout1')(x)\n",
    "  x = Conv1D(filters = 64, kernel_size = 5, strides = 2, padding = 'valid', activation='relu', name='conv3')(x)\n",
    "  x = Conv1D(filters = 64, kernel_size = 5, strides = 2, padding = 'valid', activation='relu', name='conv4')(x)\n",
    "  x = AvgPool1D(pool_size=2, name='avg_pool2')(x)\n",
    "  x = BatchNormalization(name='batch_norm2')(x)\n",
    "  x = SpatialDropout1D(rate=0.125, name = 'spatial_dropout2')(x)\n",
    "  x = Conv1D(filters = 128, kernel_size = 3, strides = 1, padding = 'valid', activation='relu', name='conv5')(x)\n",
    "  x = Conv1D(filters = 128, kernel_size = 3, strides = 1, padding = 'valid', activation='relu', name='conv6')(x)\n",
    "  x = AvgPool1D(pool_size=2, name='avg_pool3')(x)\n",
    "  x = BatchNormalization(name='batch_norm3')(x)\n",
    "  x = SpatialDropout1D(rate=0.25, name = 'spatial_dropout3')(x)\n",
    "  x = Conv1D(filters = 256, kernel_size = 3, strides = 1, padding = 'valid', activation='relu', name='conv7')(x)\n",
    "  x = Conv1D(filters = 256, kernel_size = 3, strides = 1, padding = 'valid', activation='relu', name='conv8')(x)\n",
    "  x = GlobalAvgPool1D(name='global_pool1')(x)\n",
    "  x = BatchNormalization(name='batch_norm4')(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  x = Dense(64)(x)\n",
    "  x = Activation('tanh')(x)\n",
    "  x = Dense(8)(x)\n",
    "  x = Activation('tanh')(x)\n",
    "  x = Dropout(0.1)(x)\n",
    "  return x\n",
    "\n",
    "def get_model():\n",
    "  x = get_CNN_design()\n",
    "\n",
    "  out = Dense(classes, activation='softmax', name = 'output')(x) # Output layer\n",
    "\n",
    "  model = Model(inputs=eeg_input, outputs=out) # Creating a model instance\n",
    "  \n",
    "  adam = Adam(lr=1e-3,decay=1e-5) # Adam optimizer\n",
    "  \n",
    "  model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['categorical_accuracy']) # Compiling model\n",
    "  model.summary()\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELFpIRMfCvyO"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LL1xiSVVlbm",
    "outputId": "bd4d01d0-cfb0-44c8-d1d9-5096d70713c0"
   },
   "outputs": [],
   "source": [
    "val_res = {'accuracy': [], 'confusion_matrix': []}\n",
    "\n",
    "gc.collect() # Garbage collecter\n",
    "for i in range(FOLD):\n",
    "  print('Fold: '+str(i))\n",
    "\n",
    "  X_train = np.concatenate((data[0 : i*factor], data[(i+1)*factor : nb_samples])) # Training data\n",
    "  X_test = data[i*factor : (i+1)*factor] # Testing data\n",
    "  val_train = np.concatenate((valence[0 : i*factor], valence[(i+1)*factor : nb_samples])) # Valence training labels\n",
    "  val_test = valence[i*factor : (i+1)*factor] # Valence testing labels\n",
    "\n",
    "  gc.collect() # Garbage collecter\n",
    "  model = get_model()\n",
    "  model.fit(X_train, val_train, epochs = 100, batch_size = 1024, shuffle = True)\n",
    "\n",
    "  acc = model.evaluate(X_test, val_test)\n",
    "  print(acc)\n",
    "\n",
    "  val_res['accuracy'].append(acc)\n",
    "\n",
    "  pred = model.predict(X_test)\n",
    "  val_res['confusion_matrix'].append(confusion_matrix(val_test.argmax(1), pred.argmax(1)))\n",
    "  gc.collect() # Garbage collecter\n",
    "\n",
    "\n",
    "# Dumping valence results\n",
    "file = './eeg_data/DEAP_valence_' + eeg_band + '.pkl'\n",
    "\n",
    "with open(file, 'wb') as f:\n",
    "  pkl.dump(val_res, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_Ctf_PiZixF",
    "outputId": "2d824835-3b10-402c-d93b-e56cc3338418"
   },
   "outputs": [],
   "source": [
    "for i in val_res['accuracy']:\n",
    "  print(round(i[1]*100, 2)) # Rounding off to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqlevXjjZoP_",
    "outputId": "70f1f092-cb44-4b3e-9617-68fe2f737aa0"
   },
   "outputs": [],
   "source": [
    "for i in val_res['confusion_matrix']:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqc0PlLQaUw0",
    "outputId": "724cea0c-2110-48a5-be9f-84475aa0bce5"
   },
   "outputs": [],
   "source": [
    "aro_res = {'accuracy': [], 'confusion_matrix': []}\n",
    "\n",
    "gc.collect() # Garbage collecter\n",
    "for i in range(FOLD):\n",
    "  print('Fold: '+str(i))\n",
    "  \n",
    "  X_train = np.concatenate((data[0 : i*factor], data[(i+1)*factor : nb_samples])) # Training data\n",
    "  X_test = data[i*factor : (i+1)*factor] # Testing data\n",
    "  aro_train = np.concatenate((arousal[0 : i*factor], arousal[(i+1)*factor : nb_samples])) # Arousal training labels\n",
    "  aro_test = arousal[i*factor : (i+1)*factor] # Arousal testing labels\n",
    "\n",
    "  gc.collect() # Garbage collecter\n",
    "  model = get_model()\n",
    "  model.fit(X_train, aro_train, epochs = 100, batch_size = 1024, shuffle = True)\n",
    "\n",
    "  acc = model.evaluate(X_test, aro_test)\n",
    "  print(acc)\n",
    "\n",
    "  aro_res['accuracy'].append(acc)\n",
    "\n",
    "  pred = model.predict(X_test)\n",
    "  aro_res['confusion_matrix'].append(confusion_matrix(aro_test.argmax(1), pred.argmax(1)))\n",
    "  gc.collect() # Garbage collecter\n",
    "\n",
    "\n",
    "# Dumping arousal results\n",
    "file = './eeg_data/DEAP_arousal_' + eeg_band + '.pkl'\n",
    "\n",
    "with open(file, 'wb') as f:\n",
    "  pkl.dump(aro_res, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej7e6NkKaVLz"
   },
   "outputs": [],
   "source": [
    "for i in aro_res['accuracy']:\n",
    "  print(round(i[1]*100, 2)) # Rounding off to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwIreI3xaVQQ"
   },
   "outputs": [],
   "source": [
    "for i in aro_res['confusion_matrix']:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLbR7p7kaVUw",
    "outputId": "678149f3-128d-4f29-eb70-437a315979f0"
   },
   "outputs": [],
   "source": [
    "dom_res = {'accuracy':[], 'confusion_matrix':[]}\n",
    "\n",
    "gc.collect() # Garbage collecter\n",
    "for i in range(FOLD):\n",
    "  print('Fold: '+str(i))\n",
    "\n",
    "  X_train = np.concatenate((data[0 : i*factor], data[(i+1)*factor : nb_samples])) # Training data\n",
    "  X_test = data[i*factor : (i+1)*factor] # Testing data\n",
    "  dom_train = np.concatenate((dominance[0 : i*factor], dominance[(i+1)*factor : nb_samples])) # Dominance training labels\n",
    "  dom_test = dominance[i*factor : (i+1)*factor] # Dominance testing labels\n",
    "\n",
    "  gc.collect() # Garbage collecter\n",
    "  model = get_model()\n",
    "  model.fit(X_train, dom_train, epochs = 100, batch_size = 1024, shuffle = True)\n",
    "\n",
    "  acc = model.evaluate(X_test, dom_test)\n",
    "  print(acc)\n",
    "    \n",
    "  dom_res['accuracy'].append(acc)\n",
    "\n",
    "  pred = model.predict(X_test)\n",
    "  dom_res['confusion_matrix'].append(confusion_matrix(dom_test.argmax(1), pred.argmax(1)))\n",
    "  gc.collect() # Garbage collecter\n",
    "\n",
    "\n",
    "# Dumping dominance results\n",
    "file = './eeg_data/DEAP_dominance_' + eeg_band + '.pkl'\n",
    "\n",
    "with open(file, 'wb') as f:\n",
    "  pkl.dump(dom_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "4-8ntg33aVaW",
    "outputId": "2d815253-25d3-4cd3-9096-ff27b474659c"
   },
   "outputs": [],
   "source": [
    "for i in dom_res['accuracy']:\n",
    "  print(round(i[1]*100, 2)) # Rounding off to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGHvaJHIaVjf"
   },
   "outputs": [],
   "source": [
    "for i in dom_res['confusion_matrix']:\n",
    "  print(i)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DEAP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
